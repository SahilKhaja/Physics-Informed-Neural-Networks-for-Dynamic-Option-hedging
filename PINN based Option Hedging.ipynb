{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285ad27f",
   "metadata": {},
   "source": [
    "AMS 516 - Stochastic Volatility Option hedging\n",
    "\n",
    "Members: \n",
    "Sahil Khaja Huzoor\n",
    "\n",
    "Nick Koukounas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e143e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,math\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d46a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0] total=inf pde=1.968e+14 term=inf bc=4.018e-09\n",
      "[ 200] total=inf pde=1.713e+14 term=inf bc=4.345e-09\n",
      "[ 400] total=inf pde=1.914e+14 term=inf bc=4.123e-09\n",
      "[ 600] total=inf pde=2.383e+14 term=inf bc=4.194e-09\n",
      "[ 800] total=inf pde=2.017e+14 term=inf bc=4.111e-09\n",
      "[1000] total=inf pde=1.917e+14 term=inf bc=4.321e-09\n",
      "[1200] total=inf pde=2.897e+14 term=inf bc=4.546e-09\n",
      "[1400] total=inf pde=2.127e+14 term=inf bc=4.108e-09\n",
      "[1600] total=inf pde=1.598e+14 term=inf bc=4.739e-09\n",
      "[1800] total=inf pde=2.162e+14 term=inf bc=4.451e-09\n",
      "[2000] total=inf pde=2.599e+14 term=inf bc=4.181e-09\n",
      "[2200] total=inf pde=3.077e+14 term=inf bc=4.221e-09\n",
      "[2400] total=inf pde=2.713e+14 term=inf bc=4.568e-09\n",
      "[2600] total=inf pde=1.576e+14 term=inf bc=4.438e-09\n",
      "[2800] total=inf pde=2.062e+14 term=inf bc=4.240e-09\n"
     ]
    }
   ],
   "source": [
    "mu, r = 0.10, 0.02\n",
    "kappa, theta, xi, rho = 2.0, 0.04, 0.30, -0.70\n",
    "gamma = 2.0 # This is the risk aversion parameter under CARA utility function. For gamma > 0, the individual is risk averse, for gamma < 0, the individual is risk seeking. \n",
    "\n",
    "K, T = 100.0, 1.0 \n",
    "\n",
    "#domains of stock price and volatility\n",
    "X_min, X_max = -2.0*K, 2.0*K\n",
    "S_min, S_max = 20.0, 400.0\n",
    "v_min, v_max = 1e-4, 0.5\n",
    "\n",
    "# sample sizes \n",
    "N_int  = 8000\n",
    "N_tau0 = 2000\n",
    "N_b    = 1000\n",
    "\n",
    "# defining our payoff function\n",
    "\n",
    "def call_payoff(S):\n",
    "    return torch.clamp(S-K, min=0.0)\n",
    "\n",
    "\n",
    "def sample_uniform(n, low, high):\n",
    "    return low + (high - low) * torch.rand(n, 1, device=device)    \n",
    "    \n",
    "\n",
    "class FeedForwardNet(torch.nn.Module):\n",
    "    def __init__(self, in_dim=4, h1=128, h2=128, h3=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, h1)\n",
    "        self.fc2 = torch.nn.Linear(h1, h2)\n",
    "        self.fc3 = torch.nn.Linear(h2, h3)\n",
    "        self.out = torch.nn.Linear(h3, 1)\n",
    "        self.act = torch.nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.act(self.fc3(x))\n",
    "        return self.out(x)\n",
    "\n",
    "V_theta = FeedForwardNet().to(device)\n",
    "\n",
    "opt = torch.optim.Adam(V_theta.parameters(), lr=1e-3)\n",
    "\n",
    "def V_terminal(X, S):\n",
    "    return -torch.exp(-gamma * (X - call_payoff(S)))\n",
    "\n",
    "def V_and_grads(tau,X,S,v):\n",
    "    tau.requires_grad_(True);S.requires_grad_(True);v.requires_grad_(True);X.requires_grad_(True)\n",
    "\n",
    "    x = torch.cat([tau,X,S,v],dim = 1)\n",
    "    V = V_theta(x) # My value function that is being approximated with all the parameters\n",
    "\n",
    "    V_tau = torch.autograd.grad(V,tau,grad_outputs=torch.ones_like(V),retain_graph=True,create_graph=True)[0]\n",
    "    V_X   = torch.autograd.grad(V, X,   grad_outputs=torch.ones_like(V),retain_graph=True, create_graph=True)[0]\n",
    "    V_S   = torch.autograd.grad(V, S,   grad_outputs=torch.ones_like(V),retain_graph=True, create_graph=True)[0]\n",
    "    V_v   = torch.autograd.grad(V, v,   grad_outputs=torch.ones_like(V),retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    V_XX  = torch.autograd.grad(V_X, X, grad_outputs=torch.ones_like(V_X),retain_graph=True, create_graph=True)[0]\n",
    "    V_SS  = torch.autograd.grad(V_S, S, grad_outputs=torch.ones_like(V_S),retain_graph=True, create_graph=True)[0]\n",
    "    V_vv  = torch.autograd.grad(V_v, v, grad_outputs=torch.ones_like(V_v),retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    V_XS  = torch.autograd.grad(V_X, S, grad_outputs=torch.ones_like(V_X),retain_graph=True, create_graph=True)[0]\n",
    "    V_Xv  = torch.autograd.grad(V_X, v, grad_outputs=torch.ones_like(V_X),retain_graph=True, create_graph=True)[0]\n",
    "    V_Sv  = torch.autograd.grad(V_S, v, grad_outputs=torch.ones_like(V_S),retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    return V, V_tau, V_X, V_S, V_v, V_XX, V_SS, V_vv, V_XS, V_Xv, V_Sv\n",
    "\n",
    "def hjb_residual(tau,X,S,v):\n",
    "    V, V_tau, V_X, V_S, V_v, V_XX, V_SS, V_vv, V_XS, V_Xv, V_Sv = V_and_grads(tau, X, S, v)\n",
    "    sqrtv = torch.sqrt(torch.clamp(v,min=1e-8))\n",
    "\n",
    "    drift_X = r * X * V_X\n",
    "    drift_v = kappa * (theta - v) * V_v\n",
    "    diff_S  = 0.5 * v * S**2 * V_SS\n",
    "    diff_v  = 0.5 * xi**2 * v * V_vv\n",
    "    mix_Sv  = rho * xi * S * torch.clamp(v,min=1e-8) * V_Sv\n",
    "    # These are all the deterministic parts of the HJB equation\n",
    "\n",
    "\n",
    "    # This is the optimization over pi term\n",
    "    num = (mu - r) * V_X + v * S * V_XS + rho * xi * v * V_Xv\n",
    "    den = 2.0 * torch.clamp(v * V_XX, min=1e-10)\n",
    "    control = (num**2) / den\n",
    "\n",
    "    # Final HJB term\n",
    "    R = -V_tau + drift_X + drift_v + diff_S + diff_v + mix_Sv - control\n",
    "    return R, V\n",
    "\n",
    "def boundary_loss(m=N_b): # This code is for boundary condition of Stock and volatility. We essentially say that the value function will barely change when there is a change in stock prices at low and high values of the stock price since they are either deep in the money or deep out of the money \n",
    "\n",
    "    # This is the Neumann boundary condition: ie value of the derivative of a function at its boundary, also known as flux\n",
    "\n",
    "    # picking random points for time, wealth and variance inside the domain \n",
    "    tau_b = sample_uniform(m, 0.0, T)   \n",
    "    X_b   = sample_uniform(m, X_min, X_max)\n",
    "    v_b   = sample_uniform(m, v_min, v_max)\n",
    "    # These are points along the boundary walls of our training domain\n",
    "\n",
    "    # We are fixing Stock price to be next to its boundary values\n",
    "\n",
    "\n",
    "    # Computing The slopes of V at low and high stock price boundaries\n",
    "    S_lo = torch.full_like(tau_b,S_min) # Creating a tensor of just the minimum value of the stock price\n",
    "    S_hi = torch.full_like(tau_b,S_max) # Creating a tensor of just the maximum value of the stock price\n",
    "\n",
    "    _,_,_,V_S_lo,_,_,_,_,_,_,_ = V_and_grads(tau_b,X_b,S_lo,v_b)\n",
    "    _,_,_,V_S_hi,_,_,_,_,_,_,_ = V_and_grads(tau_b,X_b,S_hi,v_b)\n",
    "\n",
    "    loss_S = 1e-4*((V_S_lo**2).mean() + (V_S_hi**2).mean())\n",
    "\n",
    "\n",
    "    # repeating for volatility boundary \n",
    "    tau_b2 = sample_uniform(m, 0.0, T)\n",
    "    S_b2   = sample_uniform(m, S_min, S_max)\n",
    "    X_b2   = sample_uniform(m, X_min, X_max)\n",
    "    v_lo   = torch.full_like(tau_b2, v_min)\n",
    "    v_hi   = torch.full_like(tau_b2, v_max)\n",
    "    _,_,_,_,V_v_lo,_,_,_,_,_,_ = V_and_grads(tau_b2,X_b2,S_b2,v_lo)\n",
    "    _,_,_,_,V_v_hi,_,_,_,_,_,_ = V_and_grads(tau_b2,X_b2,S_b2,v_hi)\n",
    "    loss_v = 1e-4*((V_v_lo**2).mean() + (V_v_hi**2).mean())\n",
    "\n",
    "    return loss_S+loss_v\n",
    "\n",
    "\n",
    "def train(steps=3000):\n",
    "    for it in range(steps):\n",
    "\n",
    "        # This is the Dirichlet condition: ie value of the function at time boundary \n",
    "        tau = sample_uniform(N_int, 0.0, T)\n",
    "        X   = sample_uniform(N_int, X_min, X_max)\n",
    "        S   = sample_uniform(N_int, S_min, S_max)\n",
    "        v   = sample_uniform(N_int, v_min, v_max)\n",
    "\n",
    "        # Computing the PDE residual with the random points simulated\n",
    "\n",
    "        R,_ = hjb_residual(tau, X, S, v)\n",
    "        loss_pde = (R**2).mean() # mean square PDE residual\n",
    "\n",
    "        # Terminal condition \n",
    "\n",
    "        # This code is for boundary condition of time \n",
    "        tau0 = torch.zeros(N_tau0, 1, device=device)\n",
    "        X0   = sample_uniform(N_tau0, X_min, X_max)\n",
    "        S0   = sample_uniform(N_tau0, S_min, S_max)\n",
    "        v0   = sample_uniform(N_tau0, v_min, v_max)\n",
    "\n",
    "        V_T_pred = V_theta(torch.cat([tau0,X0,S0,v0],dim=1))\n",
    "        V_T_true = V_terminal(X0,S0)\n",
    "        loss_term = ((V_T_pred - V_T_true)**2).mean()\n",
    "\n",
    "        # Enforcing the boundary condition\n",
    "        loss_bc = boundary_loss()\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_term + loss_bc + loss_pde\n",
    "\n",
    "\n",
    "        # backpropagating the loss function\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(V_theta.parameters(),5.0)\n",
    "\n",
    "\n",
    "        if it % 200 == 0:\n",
    "            print(f\"[{it:4d}] total={loss.item():.3e} pde={loss_pde.item():.3e} term={loss_term.item():.3e} bc={loss_bc.item():.3e}\")\n",
    "\n",
    "\n",
    "train(steps=3000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9c3eb",
   "metadata": {},
   "source": [
    "Modifying the code for stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ff368e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0] total=4.000e+34 pde=7.527e-01 term=4.000e+34 bc=4.365e-09\n",
      "[ 200] total=3.985e+34 pde=6.994e-01 term=3.985e+34 bc=4.444e-09\n",
      "[ 400] total=4.065e+34 pde=7.554e-01 term=4.065e+34 bc=4.367e-09\n",
      "[ 600] total=4.028e+34 pde=7.181e-01 term=4.028e+34 bc=4.060e-09\n",
      "[ 800] total=4.044e+34 pde=7.365e-01 term=4.044e+34 bc=4.555e-09\n",
      "[1000] total=4.009e+34 pde=7.530e-01 term=4.009e+34 bc=4.484e-09\n",
      "[1200] total=4.018e+34 pde=7.517e-01 term=4.018e+34 bc=3.940e-09\n",
      "[1400] total=4.067e+34 pde=7.493e-01 term=4.067e+34 bc=4.265e-09\n",
      "[1600] total=3.976e+34 pde=7.190e-01 term=3.976e+34 bc=4.225e-09\n",
      "[1800] total=4.006e+34 pde=6.933e-01 term=4.006e+34 bc=4.603e-09\n",
      "[2000] total=3.968e+34 pde=7.066e-01 term=3.968e+34 bc=4.137e-09\n",
      "[2200] total=3.998e+34 pde=6.860e-01 term=3.998e+34 bc=4.147e-09\n",
      "[2400] total=3.963e+34 pde=6.899e-01 term=3.963e+34 bc=4.287e-09\n",
      "[2600] total=4.000e+34 pde=7.817e-01 term=4.000e+34 bc=4.405e-09\n",
      "[2800] total=3.961e+34 pde=7.737e-01 term=3.961e+34 bc=4.342e-09\n",
      "[3000] total=3.980e+34 pde=7.392e-01 term=3.980e+34 bc=4.458e-09\n",
      "[3200] total=4.002e+34 pde=7.385e-01 term=4.002e+34 bc=4.512e-09\n",
      "[3400] total=3.905e+34 pde=8.046e-01 term=3.905e+34 bc=4.291e-09\n",
      "[3600] total=3.977e+34 pde=7.490e-01 term=3.977e+34 bc=4.147e-09\n",
      "[3800] total=4.130e+34 pde=7.202e-01 term=4.130e+34 bc=4.406e-09\n",
      "[4000] total=4.070e+34 pde=7.595e-01 term=4.070e+34 bc=4.344e-09\n",
      "[4200] total=4.001e+34 pde=7.479e-01 term=4.001e+34 bc=4.063e-09\n",
      "[4400] total=4.046e+34 pde=7.223e-01 term=4.046e+34 bc=4.360e-09\n",
      "[4600] total=4.049e+34 pde=7.440e-01 term=4.049e+34 bc=4.309e-09\n",
      "[4800] total=4.047e+34 pde=7.316e-01 term=4.047e+34 bc=4.105e-09\n",
      "[5000] total=4.093e+34 pde=7.839e-01 term=4.093e+34 bc=4.207e-09\n",
      "[5200] total=4.066e+34 pde=7.962e-01 term=4.066e+34 bc=4.393e-09\n",
      "[5400] total=4.072e+34 pde=7.361e-01 term=4.072e+34 bc=4.216e-09\n",
      "[5600] total=4.062e+34 pde=7.719e-01 term=4.062e+34 bc=4.212e-09\n",
      "[5800] total=4.155e+34 pde=7.263e-01 term=4.155e+34 bc=4.173e-09\n",
      "[6000] total=4.088e+34 pde=7.362e-01 term=4.088e+34 bc=3.985e-09\n",
      "[6200] total=4.012e+34 pde=7.719e-01 term=4.012e+34 bc=4.251e-09\n",
      "[6400] total=3.967e+34 pde=7.453e-01 term=3.967e+34 bc=4.272e-09\n",
      "[6600] total=3.996e+34 pde=7.830e-01 term=3.996e+34 bc=4.160e-09\n",
      "[6800] total=3.979e+34 pde=7.431e-01 term=3.979e+34 bc=4.141e-09\n",
      "[7000] total=4.037e+34 pde=7.484e-01 term=4.037e+34 bc=3.905e-09\n",
      "[7200] total=4.057e+34 pde=7.511e-01 term=4.057e+34 bc=4.573e-09\n",
      "[7400] total=4.060e+34 pde=7.669e-01 term=4.060e+34 bc=3.990e-09\n",
      "[7600] total=4.045e+34 pde=7.538e-01 term=4.045e+34 bc=4.501e-09\n",
      "[7800] total=3.991e+34 pde=7.634e-01 term=3.991e+34 bc=4.727e-09\n",
      "[8000] total=4.061e+34 pde=7.986e-01 term=4.061e+34 bc=4.114e-09\n",
      "[8200] total=4.044e+34 pde=7.280e-01 term=4.044e+34 bc=4.107e-09\n",
      "[8400] total=4.038e+34 pde=7.297e-01 term=4.038e+34 bc=4.304e-09\n",
      "[8600] total=4.081e+34 pde=7.073e-01 term=4.081e+34 bc=4.598e-09\n",
      "[8800] total=3.993e+34 pde=7.544e-01 term=3.993e+34 bc=4.495e-09\n",
      "[9000] total=4.069e+34 pde=7.380e-01 term=4.069e+34 bc=4.150e-09\n",
      "[9200] total=4.055e+34 pde=7.619e-01 term=4.055e+34 bc=4.270e-09\n",
      "[9400] total=3.983e+34 pde=7.395e-01 term=3.983e+34 bc=4.259e-09\n",
      "[9600] total=4.109e+34 pde=7.333e-01 term=4.109e+34 bc=4.528e-09\n",
      "[9800] total=4.021e+34 pde=7.992e-01 term=4.021e+34 bc=4.371e-09\n",
      "[10000] total=3.966e+34 pde=7.873e-01 term=3.966e+34 bc=4.240e-09\n",
      "[10200] total=4.032e+34 pde=7.423e-01 term=4.032e+34 bc=4.391e-09\n",
      "[10400] total=4.053e+34 pde=6.960e-01 term=4.053e+34 bc=4.413e-09\n",
      "[10600] total=3.929e+34 pde=7.936e-01 term=3.929e+34 bc=4.229e-09\n",
      "[10800] total=3.930e+34 pde=7.349e-01 term=3.930e+34 bc=4.533e-09\n",
      "[11000] total=4.124e+34 pde=7.370e-01 term=4.124e+34 bc=4.520e-09\n",
      "[11200] total=4.057e+34 pde=7.306e-01 term=4.057e+34 bc=4.364e-09\n",
      "[11400] total=3.970e+34 pde=7.409e-01 term=3.970e+34 bc=4.180e-09\n",
      "[11600] total=4.040e+34 pde=8.123e-01 term=4.040e+34 bc=4.740e-09\n",
      "[11800] total=3.974e+34 pde=7.295e-01 term=3.974e+34 bc=3.961e-09\n",
      "[12000] total=4.045e+34 pde=7.408e-01 term=4.045e+34 bc=4.231e-09\n",
      "[12200] total=4.075e+34 pde=7.476e-01 term=4.075e+34 bc=4.220e-09\n",
      "[12400] total=4.015e+34 pde=7.852e-01 term=4.015e+34 bc=3.990e-09\n",
      "[12600] total=4.056e+34 pde=7.827e-01 term=4.056e+34 bc=4.135e-09\n",
      "[12800] total=4.031e+34 pde=7.729e-01 term=4.031e+34 bc=4.265e-09\n",
      "[13000] total=4.148e+34 pde=7.960e-01 term=4.148e+34 bc=4.286e-09\n",
      "[13200] total=4.098e+34 pde=7.270e-01 term=4.098e+34 bc=4.261e-09\n",
      "[13400] total=4.003e+34 pde=7.073e-01 term=4.003e+34 bc=4.172e-09\n",
      "[13600] total=4.036e+34 pde=6.912e-01 term=4.036e+34 bc=4.210e-09\n",
      "[13800] total=4.086e+34 pde=7.909e-01 term=4.086e+34 bc=4.244e-09\n",
      "[14000] total=3.971e+34 pde=6.997e-01 term=3.971e+34 bc=4.264e-09\n",
      "[14200] total=3.965e+34 pde=7.351e-01 term=3.965e+34 bc=4.277e-09\n",
      "[14400] total=4.116e+34 pde=7.574e-01 term=4.116e+34 bc=3.851e-09\n",
      "[14600] total=4.120e+34 pde=7.209e-01 term=4.120e+34 bc=4.198e-09\n",
      "[14800] total=3.960e+34 pde=7.484e-01 term=3.960e+34 bc=4.108e-09\n",
      "[15000] total=4.031e+34 pde=6.766e-01 term=4.031e+34 bc=3.788e-09\n",
      "[15200] total=3.971e+34 pde=7.092e-01 term=3.971e+34 bc=4.470e-09\n",
      "[15400] total=3.979e+34 pde=7.442e-01 term=3.979e+34 bc=4.395e-09\n",
      "[15600] total=3.992e+34 pde=7.513e-01 term=3.992e+34 bc=4.140e-09\n",
      "[15800] total=3.909e+34 pde=7.420e-01 term=3.909e+34 bc=4.297e-09\n",
      "[16000] total=3.985e+34 pde=7.168e-01 term=3.985e+34 bc=4.576e-09\n",
      "[16200] total=3.919e+34 pde=7.335e-01 term=3.919e+34 bc=4.210e-09\n",
      "[16400] total=4.071e+34 pde=8.022e-01 term=4.071e+34 bc=4.148e-09\n",
      "[16600] total=4.039e+34 pde=7.712e-01 term=4.039e+34 bc=4.473e-09\n",
      "[16800] total=3.936e+34 pde=7.203e-01 term=3.936e+34 bc=4.012e-09\n",
      "[17000] total=4.042e+34 pde=7.289e-01 term=4.042e+34 bc=4.299e-09\n",
      "[17200] total=4.057e+34 pde=7.290e-01 term=4.057e+34 bc=4.534e-09\n",
      "[17400] total=3.983e+34 pde=7.520e-01 term=3.983e+34 bc=4.615e-09\n",
      "[17600] total=3.959e+34 pde=7.124e-01 term=3.959e+34 bc=4.066e-09\n",
      "[17800] total=3.989e+34 pde=7.026e-01 term=3.989e+34 bc=4.778e-09\n",
      "[18000] total=4.023e+34 pde=7.402e-01 term=4.023e+34 bc=4.621e-09\n",
      "[18200] total=3.959e+34 pde=7.460e-01 term=3.959e+34 bc=4.126e-09\n",
      "[18400] total=4.102e+34 pde=7.465e-01 term=4.102e+34 bc=4.390e-09\n",
      "[18600] total=3.965e+34 pde=7.762e-01 term=3.965e+34 bc=3.994e-09\n",
      "[18800] total=3.958e+34 pde=7.189e-01 term=3.958e+34 bc=4.458e-09\n",
      "[19000] total=4.049e+34 pde=7.842e-01 term=4.049e+34 bc=4.258e-09\n",
      "[19200] total=3.983e+34 pde=7.694e-01 term=3.983e+34 bc=4.473e-09\n",
      "[19400] total=4.028e+34 pde=7.486e-01 term=4.028e+34 bc=4.383e-09\n",
      "[19600] total=3.975e+34 pde=7.377e-01 term=3.975e+34 bc=4.094e-09\n",
      "[19800] total=3.936e+34 pde=7.685e-01 term=3.936e+34 bc=4.349e-09\n",
      "[20000] total=4.092e+34 pde=7.141e-01 term=4.092e+34 bc=4.155e-09\n",
      "[20200] total=3.977e+34 pde=7.766e-01 term=3.977e+34 bc=4.618e-09\n",
      "[20400] total=3.998e+34 pde=7.125e-01 term=3.998e+34 bc=4.343e-09\n",
      "[20600] total=3.908e+34 pde=7.313e-01 term=3.908e+34 bc=4.098e-09\n",
      "[20800] total=3.992e+34 pde=7.065e-01 term=3.992e+34 bc=4.307e-09\n",
      "[21000] total=3.989e+34 pde=7.670e-01 term=3.989e+34 bc=4.176e-09\n",
      "[21200] total=3.950e+34 pde=7.568e-01 term=3.950e+34 bc=4.209e-09\n",
      "[21400] total=3.995e+34 pde=7.216e-01 term=3.995e+34 bc=4.503e-09\n",
      "[21600] total=4.073e+34 pde=7.356e-01 term=4.073e+34 bc=4.481e-09\n",
      "[21800] total=4.034e+34 pde=7.440e-01 term=4.034e+34 bc=4.250e-09\n",
      "[22000] total=3.957e+34 pde=7.262e-01 term=3.957e+34 bc=4.476e-09\n",
      "[22200] total=4.027e+34 pde=7.516e-01 term=4.027e+34 bc=4.196e-09\n",
      "[22400] total=4.074e+34 pde=7.937e-01 term=4.074e+34 bc=4.338e-09\n",
      "[22600] total=4.009e+34 pde=7.371e-01 term=4.009e+34 bc=4.157e-09\n",
      "[22800] total=3.977e+34 pde=7.588e-01 term=3.977e+34 bc=4.408e-09\n",
      "[23000] total=4.089e+34 pde=7.474e-01 term=4.089e+34 bc=4.392e-09\n",
      "[23200] total=3.982e+34 pde=7.632e-01 term=3.982e+34 bc=4.462e-09\n",
      "[23400] total=4.005e+34 pde=8.028e-01 term=4.005e+34 bc=4.183e-09\n",
      "[23600] total=4.114e+34 pde=7.357e-01 term=4.114e+34 bc=4.325e-09\n",
      "[23800] total=4.019e+34 pde=7.346e-01 term=4.019e+34 bc=4.483e-09\n",
      "[24000] total=4.007e+34 pde=7.809e-01 term=4.007e+34 bc=4.434e-09\n",
      "[24200] total=4.077e+34 pde=7.431e-01 term=4.077e+34 bc=4.495e-09\n",
      "[24400] total=4.043e+34 pde=7.462e-01 term=4.043e+34 bc=4.345e-09\n",
      "[24600] total=4.050e+34 pde=7.781e-01 term=4.050e+34 bc=4.357e-09\n",
      "[24800] total=3.982e+34 pde=7.105e-01 term=3.982e+34 bc=4.529e-09\n",
      "[25000] total=3.997e+34 pde=7.386e-01 term=3.997e+34 bc=4.249e-09\n",
      "[25200] total=3.946e+34 pde=7.432e-01 term=3.946e+34 bc=4.287e-09\n",
      "[25400] total=4.028e+34 pde=7.450e-01 term=4.028e+34 bc=4.495e-09\n",
      "[25600] total=3.971e+34 pde=7.320e-01 term=3.971e+34 bc=4.101e-09\n",
      "[25800] total=3.994e+34 pde=7.702e-01 term=3.994e+34 bc=4.282e-09\n",
      "[26000] total=3.926e+34 pde=7.274e-01 term=3.926e+34 bc=4.095e-09\n",
      "[26200] total=4.061e+34 pde=7.558e-01 term=4.061e+34 bc=4.264e-09\n",
      "[26400] total=4.054e+34 pde=7.493e-01 term=4.054e+34 bc=3.942e-09\n",
      "[26600] total=4.022e+34 pde=7.338e-01 term=4.022e+34 bc=3.890e-09\n",
      "[26800] total=4.105e+34 pde=7.287e-01 term=4.105e+34 bc=4.585e-09\n",
      "[27000] total=4.045e+34 pde=7.278e-01 term=4.045e+34 bc=4.174e-09\n",
      "[27200] total=4.012e+34 pde=7.502e-01 term=4.012e+34 bc=4.326e-09\n",
      "[27400] total=4.095e+34 pde=7.182e-01 term=4.095e+34 bc=4.718e-09\n",
      "[27600] total=3.942e+34 pde=7.713e-01 term=3.942e+34 bc=4.242e-09\n",
      "[27800] total=4.001e+34 pde=7.416e-01 term=4.001e+34 bc=4.344e-09\n",
      "[28000] total=4.015e+34 pde=7.812e-01 term=4.015e+34 bc=4.582e-09\n",
      "[28200] total=3.982e+34 pde=7.523e-01 term=3.982e+34 bc=4.147e-09\n",
      "[28400] total=4.100e+34 pde=7.476e-01 term=4.100e+34 bc=4.438e-09\n",
      "[28600] total=3.985e+34 pde=7.635e-01 term=3.985e+34 bc=4.332e-09\n",
      "[28800] total=4.065e+34 pde=7.648e-01 term=4.065e+34 bc=4.463e-09\n",
      "[29000] total=4.098e+34 pde=7.482e-01 term=4.098e+34 bc=4.205e-09\n",
      "[29200] total=4.001e+34 pde=7.170e-01 term=4.001e+34 bc=4.399e-09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     21\u001b[39m     R = -V_tau + drift_X + drift_v + diff_S + diff_v + mix_Sv - control\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m R, V\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(steps)\u001b[39m\n\u001b[32m    147\u001b[39m loss_term = ((V_T_pred - V_T_true)**\u001b[32m2\u001b[39m).mean()\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# Enforcing the boundary condition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m loss_bc = \u001b[43mboundary_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Total loss\u001b[39;00m\n\u001b[32m    153\u001b[39m loss = loss_term + loss_bc + loss_pde\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mboundary_loss\u001b[39m\u001b[34m(m)\u001b[39m\n\u001b[32m    116\u001b[39m v_lo   = torch.full_like(tau_b2, v_min)\n\u001b[32m    117\u001b[39m v_hi   = torch.full_like(tau_b2, v_max)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m _,_,_,_,V_v_lo,_,_,_,_,_,_ = \u001b[43mV_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau_b2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_b2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS_b2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv_lo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m _,_,_,_,V_v_hi,_,_,_,_,_,_ = V_and_grads(tau_b2,X_b2,S_b2,v_hi)\n\u001b[32m    120\u001b[39m loss_v = \u001b[32m1e-4\u001b[39m*((V_v_lo**\u001b[32m2\u001b[39m).mean() + (V_v_hi**\u001b[32m2\u001b[39m).mean())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mV_and_grads\u001b[39m\u001b[34m(tau, X, S, v)\u001b[39m\n\u001b[32m     61\u001b[39m V_SS  = torch.autograd.grad(V_S, S, grad_outputs=torch.ones_like(V_S),retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     62\u001b[39m V_vv  = torch.autograd.grad(V_v, v, grad_outputs=torch.ones_like(V_v),retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m V_XS  = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV_X\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     65\u001b[39m V_Xv  = torch.autograd.grad(V_X, v, grad_outputs=torch.ones_like(V_X),retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     66\u001b[39m V_Sv  = torch.autograd.grad(V_S, v, grad_outputs=torch.ones_like(V_S),retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def V_terminal(X, S):\n",
    "    payoff = torch.clamp(X - call_payoff(S), min=-20, max=20)\n",
    "    return -torch.exp(-gamma * payoff)\n",
    "\n",
    "def hjb_residual(tau, X, S, v):\n",
    "    V, V_tau, V_X, V_S, V_v, V_XX, V_SS, V_vv, V_XS, V_Xv, V_Sv = V_and_grads(tau, X, S, v)\n",
    "    sqrtv = torch.sqrt(torch.clamp(v, min=1e-8))\n",
    "\n",
    "    # Drift and diffusion terms\n",
    "    drift_X = r * X * V_X\n",
    "    drift_v = kappa * (theta - v) * V_v\n",
    "    diff_S  = 0.5 * v * S**2 * V_SS\n",
    "    diff_v  = 0.5 * xi**2 * v * V_vv\n",
    "    mix_Sv  = rho * xi * S * sqrtv * V_Sv\n",
    "\n",
    "    # Optimal control term â€” stabilize denominator\n",
    "    num = (mu - r) * V_X + v * S * V_XS + rho * xi * v * V_Xv\n",
    "    den = 2.0 * torch.sign(V_XX) * torch.clamp(torch.abs(v * V_XX), min=1e-3)\n",
    "    control = (num**2) / den\n",
    "\n",
    "    R = -V_tau + drift_X + drift_v + diff_S + diff_v + mix_Sv - control\n",
    "    return R, V\n",
    "\n",
    "\n",
    "train(steps=30000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
